{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9151908,"sourceType":"datasetVersion","datasetId":5528372},{"sourceId":9234860,"sourceType":"datasetVersion","datasetId":5585857}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install musdb\n!pip install librosa\n!pip install museval\n!pip install wandb\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-25T12:51:19.707187Z","iopub.execute_input":"2024-08-25T12:51:19.707481Z","iopub.status.idle":"2024-08-25T12:52:10.985203Z","shell.execute_reply.started":"2024-08-25T12:51:19.707454Z","shell.execute_reply":"2024-08-25T12:52:10.984103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport torch\n\n# Check for GPU using TensorFlow\nphysical_devices = tf.config.list_physical_devices()\nprint(\"Available physical devices (TensorFlow):\", physical_devices)\n\ngpu_devices = tf.config.list_physical_devices('GPU')\nif gpu_devices:\n    print(\"GPU is available (TensorFlow)\")\nelse:\n    print(\"GPU is not available (TensorFlow)\")\n\n# Check for GPU using PyTorch\nif torch.cuda.is_available():\n    print(\"GPU is available (PyTorch)\")\n    print(\"CUDA version:\", torch.version.cuda)\n    print(\"Number of GPUs:\", torch.cuda.device_count())\n    print(\"GPU name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"GPU is not available (PyTorch)\")\n\n# Check for TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU is available')\n    print('TPU devices:', tpu.master())\nexcept ValueError:\n    print('TPU is not available')\n\n# System information\nprint(\"System information:\")\n!lscpu\n\n# GPU information\nprint(\"GPU information:\")\n!nvidia-smi\n\n# Full hardware information\nprint(\"Full hardware information:\")\n!lshw -short\n\n\n\ndevice_count = torch.cuda.device_count()\nprint(f\"Number of available GPUs: {device_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:10.986951Z","iopub.execute_input":"2024-08-25T12:52:10.987265Z","iopub.status.idle":"2024-08-25T12:52:29.590646Z","shell.execute_reply.started":"2024-08-25T12:52:10.987233Z","shell.execute_reply":"2024-08-25T12:52:29.589333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nprint(\"Current working directory:\", os.getcwd())\nprint(\"Contents of root directory:\", os.listdir('/'))\nprint(\"Contents of /kaggle:\", os.listdir('/kaggle'))\nprint(\"Contents of /kaggle/input:\", os.listdir('/kaggle/input'))\n\n# Try accessing your dataset directory again\ndataset_path = '/kaggle/input/my-data'\nif os.path.exists(dataset_path):\n    print(\"Dataset path verified:\", dataset_path)\n    print(\"Contents of dataset path:\", os.listdir(dataset_path))\nelse:\n    print(\"Dataset path does not exist:\", dataset_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:29.592165Z","iopub.execute_input":"2024-08-25T12:52:29.593176Z","iopub.status.idle":"2024-08-25T12:52:29.601148Z","shell.execute_reply.started":"2024-08-25T12:52:29.593108Z","shell.execute_reply":"2024-08-25T12:52:29.600187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef list_directory_contents(path, level=0, max_items=3):\n    if not os.path.exists(path):\n        print(f\"Path does not exist: {path}\")\n        return\n    \n    indent = '│   ' * level\n    entries = os.listdir(path)\n    entries.sort()\n\n    dirs = [entry for entry in entries if os.path.isdir(os.path.join(path, entry))]\n    files = [entry for entry in entries if os.path.isfile(os.path.join(path, entry))]\n\n    # Print directories\n    for i, d in enumerate(dirs):\n        print(f\"{indent}{'└──' if i == len(dirs) - 1 and not files else '├──'} {d}/\")\n        list_directory_contents(os.path.join(path, d), level + 1, max_items)\n\n    # Print files, limit to max_items at the deepest level\n    for j, f in enumerate(files[:max_items]):\n        print(f\"{indent}{'└──' if j == len(files[:max_items]) - 1 else '├──'} File: {f}\")\n\nlist_directory_contents('/kaggle/input/audio-seperation-dataset')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:29.603692Z","iopub.execute_input":"2024-08-25T12:52:29.604303Z","iopub.status.idle":"2024-08-25T12:52:29.697156Z","shell.execute_reply.started":"2024-08-25T12:52:29.604270Z","shell.execute_reply":"2024-08-25T12:52:29.696326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 1: Imports, Configuration, and Logger Setup\n\nimport os\nimport sys\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport librosa\nimport musdb\nfrom tqdm import tqdm\n\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Colored log formatter.\"\"\"\n    \n    COLORS = {\n        'DEBUG': '\\033[0;36m',  # Cyan\n        'INFO': '\\033[0;32m',   # Green\n        'WARNING': '\\033[0;33m',  # Yellow\n        'ERROR': '\\033[0;31m',  # Red\n        'CRITICAL': '\\033[0;37;41m',  # White on Red\n        'RESET': '\\033[0m',  # Reset\n    }\n\n    def format(self, record):\n        log_message = super().format(record)\n        return f\"{self.COLORS.get(record.levelname, self.COLORS['RESET'])}{log_message}{self.COLORS['RESET']}\"\n\ndef setup_logger(name, log_file, level=logging.DEBUG):\n    \"\"\"Function to set up a logger with both colored console and file output\"\"\"\n    \n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    \n    # Remove all existing handlers\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n    \n    # Console handler with color\n    c_handler = logging.StreamHandler(sys.stdout)\n    c_formatter = ColoredFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    c_handler.setFormatter(c_formatter)\n    c_handler.setLevel(level)\n    \n    # File handler (no color)\n    f_handler = RotatingFileHandler(log_file, maxBytes=10000, backupCount=1)\n    f_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    f_handler.setFormatter(f_formatter)\n    f_handler.setLevel(level)\n    \n    logger.addHandler(c_handler)\n    logger.addHandler(f_handler)\n    \n    return logger\n\n# Set up the logger\nlogger = setup_logger('my_logger', 'app.log', level=logging.DEBUG)\n\nclass Config:\n    SAMPLE_RATE = 44100\n    CHUNK_DURATION = 7  # in seconds\n    N_FFT = 2048\n    HOP_LENGTH = 512\n    HIDDEN_SIZE = 128\n    NUM_LAYERS = 2\n    BIDIRECTIONAL = True\n    LEARNING_RATE = 1e-3\n    BATCH_SIZE = 16\n    NUM_EPOCHS = 50\n    NUM_TRIALS = 7\n    NUM_WORKERS = 4\n    DROPOUT_RATE = 0.5\n    WEIGHT_DECAY = 0.5\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    STEMS = ['drums', 'bass', 'other', 'vocals']  # 'mixture' is always included\n    LOGGER = logger\n\n    @classmethod\n    def log_info(cls, message):\n        cls.LOGGER.info(message)\n\n# Log initial setup information\nConfig.log_info(f\"Using device: {Config.DEVICE}\")\nConfig.log_info(\"Initial setup complete.\")\n\n# Test the logger\nConfig.LOGGER.debug(\"This is a debug message\")\nConfig.LOGGER.info(\"This is an info message\")\nConfig.LOGGER.warning(\"This is a warning message\")\nConfig.LOGGER.error(\"This is an error message\")\nConfig.LOGGER.critical(\"This is a critical message\")\n\n# Explanation of libraries:\n# - librosa: Used for audio processing tasks like STFT\n# - torch: Main library for building and training neural networks\n# - musdb: Provides easy access to the MUSDB18 dataset\n# - tqdm: Used for progress bars in training loops\n\nConfig.log_info(\"All libraries imported and logger set up successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:29.698278Z","iopub.execute_input":"2024-08-25T12:52:29.698544Z","iopub.status.idle":"2024-08-25T12:52:30.042093Z","shell.execute_reply.started":"2024-08-25T12:52:29.698522Z","shell.execute_reply":"2024-08-25T12:52:30.041236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 2: Data Loading and Preprocessing\n\ndef load_musdb(root_dir):\n    \"\"\"Load the MUSDB dataset.\"\"\"\n    logger.info(f\"Loading MUSDB dataset from {root_dir}\")\n    try:\n        mus_train = musdb.DB(root_dir, subsets=['train'], is_wav=False)\n        mus_test = musdb.DB(root_dir, subsets=['test'], is_wav=False)\n        logger.info(f\"Successfully loaded {len(mus_train)} train tracks and {len(mus_test)} test tracks from MUSDB\")\n        return mus_train, mus_test\n    except Exception as e:\n        logger.error(f\"Failed to load MUSDB dataset: {str(e)}\")\n        raise\n\nclass CustomAudioMixer:\n    def __init__(self, mus, target='vocals'):\n        self.mus = mus\n        self.target = target\n        self.chunk_samples = int(Config.SAMPLE_RATE * Config.CHUNK_DURATION)\n        logger.info(\"Initialized CustomAudioMixer\")\n\n    def get_random_chunk(self, track):\n        track_duration = track.duration\n        max_start = max(track_duration - Config.CHUNK_DURATION, 0)\n        start_time = np.random.uniform(0, max_start)\n        track.chunk_start = start_time\n        track.chunk_duration = Config.CHUNK_DURATION\n\n        # Get all stems\n        stems = {stem: track.targets[stem].audio.T for stem in Config.STEMS}\n        mixture = track.audio.T\n\n        return mixture, stems\n\n    def mix(self):\n        track = np.random.choice(self.mus)\n        mixture, stems = self.get_random_chunk(track)\n        return mixture, stems[self.target]\n\nclass AudioTransforms:\n    @staticmethod\n    def to_magnitude_spectrogram(audio):\n        stft = librosa.stft(audio, n_fft=Config.N_FFT, hop_length=Config.HOP_LENGTH)\n        return np.abs(stft)\n\n    @staticmethod\n    def amplitude_to_db(spectrogram):\n        return librosa.amplitude_to_db(spectrogram, ref=np.max)\n\n    @staticmethod\n    def normalize(spectrogram):\n        epsilon = 1e-6\n        return (spectrogram - spectrogram.mean()) / (spectrogram.std() + epsilon)\n\n# Load dataset\nmusdb_root = '/kaggle/input/audio-seperation-dataset/musdb18'  # Update this path\nmus_train, mus_test = load_musdb(musdb_root)\n\nlogger.info(\"Data loading and preprocessing functions defined.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:30.043156Z","iopub.execute_input":"2024-08-25T12:52:30.043421Z","iopub.status.idle":"2024-08-25T12:52:44.073822Z","shell.execute_reply.started":"2024-08-25T12:52:30.043398Z","shell.execute_reply":"2024-08-25T12:52:44.072972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 3: Dataset Class\nclass SourceSeparationDataset(torch.utils.data.Dataset):\n    def __init__(self, mus, target='vocals'):\n        self.mus = mus\n        self.mixer = CustomAudioMixer(mus, target)\n        self.transforms = AudioTransforms()\n        self.target = target\n        logger.info(f\"Initialized SourceSeparationDataset with {len(mus)} tracks\")\n\n    def __len__(self):\n        return len(self.mus)\n\n    def __getitem__(self, idx):\n        mixture, target = self.mixer.mix()\n\n        # Process mixture\n        mixture_spec = self.transforms.to_magnitude_spectrogram(mixture)\n        mixture_spec = self.transforms.amplitude_to_db(mixture_spec)\n        mixture_spec = self.transforms.normalize(mixture_spec)\n\n        # Process target\n        target_spec = self.transforms.to_magnitude_spectrogram(target)\n        target_spec = self.transforms.amplitude_to_db(target_spec)\n        target_spec = self.transforms.normalize(target_spec)\n\n         # Only log every 20th sample\n        if idx % 20 == 0:\n            logger.debug(f\"Mixture spec shape: {mixture_spec.shape}, Target spec shape: {target_spec.shape}\")\n\n        # Ensure the output is 3D (freq, time, channels)\n        if mixture_spec.ndim == 2:\n            mixture_spec = mixture_spec[..., np.newaxis]\n            target_spec = target_spec[..., np.newaxis]\n\n        return torch.FloatTensor(mixture_spec), torch.FloatTensor(target_spec)\n\n# Create dataset instances\ntrain_dataset = SourceSeparationDataset(mus_train)\ntest_dataset = SourceSeparationDataset(mus_test)\n\n# If you want to create a validation set from the train set:\ntrain_size = int(0.8 * len(mus_train))\nval_size = len(mus_train) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n\nlogger.info(f\"Datasets created. Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)} tracks\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:44.075048Z","iopub.execute_input":"2024-08-25T12:52:44.075345Z","iopub.status.idle":"2024-08-25T12:52:44.110136Z","shell.execute_reply.started":"2024-08-25T12:52:44.075320Z","shell.execute_reply":"2024-08-25T12:52:44.109177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_spectrogram(spectrogram, title):\n    plt.figure(figsize=(10, 4))\n    if np.all(np.isnan(spectrogram)) or np.all(np.isinf(spectrogram)):\n        plt.text(0.5, 0.5, f\"Invalid Spectrogram: contains NaN or Inf\", ha='center', va='center')\n    elif np.all(spectrogram == spectrogram[0, 0]):\n        plt.text(0.5, 0.5, f\"Constant Spectrogram: all values are {spectrogram[0, 0]}\", ha='center', va='center')\n    else:\n        plt.imshow(spectrogram, aspect='auto', origin='lower', interpolation='nearest')\n    plt.title(title)\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout()\n\ndef visualize_example(dataset):\n    # Get a random sample from the dataset\n    idx = np.random.randint(len(dataset))\n    mixture_spec, target_spec = dataset[idx]\n    \n    # Convert from torch tensor to numpy array\n    mixture_spec = mixture_spec.numpy()\n    target_spec = target_spec.numpy()\n    \n    # If the spectrograms are 3D (stereo), take the first channel\n    if mixture_spec.ndim == 3:\n        mixture_spec = mixture_spec[0]\n    if target_spec.ndim == 3:\n        target_spec = target_spec[0]\n    \n    print(f\"Mixture spectrogram shape: {mixture_spec.shape}\")\n    print(f\"Mixture spectrogram min: {mixture_spec.min()}, max: {mixture_spec.max()}\")\n    print(f\"Target spectrogram shape: {target_spec.shape}\")\n    print(f\"Target spectrogram min: {target_spec.min()}, max: {target_spec.max()}\")\n    \n    # Plot spectrograms\n    plt.figure(figsize=(15, 8))\n    plt.subplot(2, 1, 1)\n    plot_spectrogram(mixture_spec, 'Mixture Spectrogram')\n    plt.subplot(2, 1, 2)\n    plot_spectrogram(target_spec, 'Target (Vocals) Spectrogram')\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualize a single random example\nlogger.info(\"Visualizing a random example from the dataset\")\nvisualize_example(train_dataset)\n\n# Visualize the distribution of spectrogram values\ndef plot_spectrogram_distribution(dataset, num_samples=1):\n    spec_values = []\n    for _ in range(num_samples):\n        mixture, target = dataset[np.random.randint(len(dataset))]\n        spec_values.extend(mixture.numpy().flatten())\n        spec_values.extend(target.numpy().flatten())\n\n    plt.figure(figsize=(10, 4))\n    plt.hist(spec_values, bins=50, density=True)\n    plt.title('Distribution of Spectrogram Values')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.tight_layout()\n    plt.show()\n\nlogger.info(\"Plotting distribution of spectrogram values\")\nplot_spectrogram_distribution(train_dataset)\n\nlogger.info(\"Dataset visualization complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:52:44.111209Z","iopub.execute_input":"2024-08-25T12:52:44.111460Z","iopub.status.idle":"2024-08-25T12:53:11.164086Z","shell.execute_reply.started":"2024-08-25T12:52:44.111439Z","shell.execute_reply":"2024-08-25T12:53:11.163161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 4: DataLoader Setup\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n\nlogger.info(\"DataLoaders created.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:53:11.165407Z","iopub.execute_input":"2024-08-25T12:53:11.166357Z","iopub.status.idle":"2024-08-25T12:53:11.173074Z","shell.execute_reply.started":"2024-08-25T12:53:11.166321Z","shell.execute_reply":"2024-08-25T12:53:11.172193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskInference(nn.Module):\n    def __init__(self):\n        super(MaskInference, self).__init__()\n        self.lstm = nn.LSTM(input_size=(Config.N_FFT // 2 + 1) * 2, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n        self.mask = nn.Linear(256, (Config.N_FFT // 2 + 1) * 2)  # Adjusted output size\n\n    def forward(self, x):\n        batch_size, channels, freq_bins, time_steps = x.size()\n\n        # Check if the input dimensions are correct\n        expected_input_size = (Config.N_FFT // 2 + 1) * 2\n        actual_input_size = freq_bins * channels\n\n        if actual_input_size != expected_input_size:\n            raise ValueError(f\"Expected input size {expected_input_size} but got {actual_input_size}\")\n\n        # Reshape for LSTM input\n        x = x.reshape(batch_size, time_steps, -1)\n        Config.LOGGER.debug(f\"Input shape after reshape: {x.shape}\")\n\n        x, _ = self.lstm(x)\n        Config.LOGGER.debug(f\"Output from LSTM shape: {x.shape}\")\n\n        # Apply mask and reshape back\n        mask = self.mask(x)\n        Config.LOGGER.debug(f\"Mask shape before final reshape: {mask.shape}\")\n\n        # Reshape to [batch_size, channels, freq_bins, time_steps]\n        mask = mask.reshape(batch_size, channels, freq_bins, time_steps)\n        return mask\n\n\nlogger.info(\"Model definition complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:53:11.176849Z","iopub.execute_input":"2024-08-25T12:53:11.177233Z","iopub.status.idle":"2024-08-25T12:53:11.187385Z","shell.execute_reply.started":"2024-08-25T12:53:11.177210Z","shell.execute_reply":"2024-08-25T12:53:11.186665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport wandb\nimport numpy as np\n\n# Set up wandb key\nwandb.login(key='af2263417a18c3258bde417967bf3c686bfa569d')\n\n# Initialize Wandb project\nwandb.init(project=\"audio_source_thesis\")\n\n# Declare global lists to store metrics\ntrain_losses = []\nval_losses = []\ntrain_sdrs = []\ntrain_sirs = []\ntrain_sars = []\n\ndef calculate_sdr(reference_signal, estimated_signal):\n    noise = reference_signal - estimated_signal\n    sdr = 10 * torch.log10(torch.sum(reference_signal ** 2) / torch.sum(noise ** 2))\n    return sdr.item()\n\ndef calculate_sir(reference_signal, estimated_signal, interference_signal):\n    interference = estimated_signal - reference_signal\n    sir = 10 * torch.log10(torch.sum(reference_signal ** 2) / torch.sum(interference_signal ** 2))\n    return sir.item()\n\ndef calculate_sar(reference_signal, estimated_signal, artifacts):\n    artifacts = estimated_signal - reference_signal\n    sar = 10 * torch.log10(torch.sum(reference_signal ** 2) / torch.sum(artifacts ** 2))\n    return sar.item()\n\ndef objective(trial):\n    global train_losses, val_losses, train_sdrs, train_sirs, train_sars\n    \n    # Suggest hyperparameters\n    Config.LEARNING_RATE = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n    Config.BATCH_SIZE = trial.suggest_categorical('batch_size', [16, 32, 64])\n    Config.HIDDEN_SIZE = trial.suggest_categorical('hidden_size', [64, 128, 256])\n    Config.NUM_LAYERS = trial.suggest_int('num_layers', 1, 4)\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n\n    # Initialize the MaskInference model with suggested hyperparameters\n    model = MaskInference().to(Config.DEVICE)\n    \n    # Log model architecture\n    wandb.watch(model)\n\n    # Optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=weight_decay)\n    criterion = nn.MSELoss()\n\n    # Learning rate scheduler\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n\n    # Data loaders\n    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=Config.NUM_WORKERS)\n\n    best_loss = float('inf')\n\n    for epoch in range(Config.NUM_EPOCHS):\n        model.train()\n        train_loss = 0.0\n\n        for batch in train_loader:\n            optimizer.zero_grad()\n\n            inputs, targets = batch\n            inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n            \n            output = model(inputs)\n            loss = criterion(output, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)  # Store training loss\n        wandb.log({\"Training Loss\": train_loss, \"Epoch\": epoch})\n\n        # Validation loop\n        val_loss = 0.0\n        sdr_list, sir_list, sar_list = [], [], []\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs, targets = batch\n                inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n\n                output = model(inputs)\n                loss = criterion(output, targets)\n                val_loss += loss.item()\n\n                # Calculate SDR, SIR, SAR\n                sdr = calculate_sdr(targets, output)\n                sir = calculate_sir(targets, output, interference_signal=inputs)\n                sar = calculate_sar(targets, output, artifacts=output - targets)\n                sdr_list.append(sdr)\n                sir_list.append(sir)\n                sar_list.append(sar)\n\n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)  # Store validation loss\n        scheduler.step(val_loss)\n\n        # Store validation metrics\n        mean_sdr = np.mean(sdr_list)\n        mean_sir = np.mean(sir_list)\n        mean_sar = np.mean(sar_list)\n        train_sdrs.append(mean_sdr)\n        train_sirs.append(mean_sir)\n        train_sars.append(mean_sar)\n\n        wandb.log({\"Validation Loss\": val_loss, \"Validation SDR\": mean_sdr, \"Validation SIR\": mean_sir, \"Validation SAR\": mean_sar, \"Epoch\": epoch})\n        Config.LOGGER.info(f'Epoch {epoch + 1}/{Config.NUM_EPOCHS} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - SDR: {mean_sdr:.4f} - SIR: {mean_sir:.4f} - SAR: {mean_sar:.4f}')\n\n        # Early stopping with Optuna\n        trial.report(val_loss, epoch)\n        if trial.should_prune():\n            Config.LOGGER.info('Trial pruned at epoch {}'.format(epoch + 1))\n            raise optuna.exceptions.TrialPruned()\n\n        # Save the best model\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            Config.LOGGER.info(f'Best model saved at epoch {epoch + 1} with loss {best_loss:.4f}')\n\n    # Return only the validation loss to Optuna\n    return val_loss\n\n# Running the optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, Config.NUM_TRIALS)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:53:11.188449Z","iopub.execute_input":"2024-08-25T12:53:11.188713Z","iopub.status.idle":"2024-08-25T12:58:39.722574Z","shell.execute_reply.started":"2024-08-25T12:53:11.188691Z","shell.execute_reply":"2024-08-25T12:58:39.721478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport yaml\nfrom torch.utils.data import DataLoader, ConcatDataset\n\n# Load the best hyperparameters from Optuna\nbest_params = study.best_trial.params\n\n# Combine the training and validation datasets\nfull_dataset = ConcatDataset([train_dataset, val_dataset])\nfull_loader = DataLoader(full_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=Config.NUM_WORKERS)\n\n# Initialize the model with the best hyperparameters\nmodel = MaskInference().to(Config.DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\ncriterion = nn.MSELoss()\n\n# Retrain the model\nfor epoch in range(Config.NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    for batch in full_loader:\n        optimizer.zero_grad()\n        inputs, targets = batch\n        inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_loss = running_loss / len(full_loader)\n    Config.LOGGER.info(f'Epoch {epoch + 1}/{Config.NUM_EPOCHS} - Loss: {avg_loss:.4f}')\n\n# Calculate final metrics on the full dataset\nmodel.eval()\nfinal_loss = avg_loss  # Final training loss\nfinal_sdr_list, final_sir_list, final_sar_list = [], [], []\n\nwith torch.no_grad():\n    for batch in full_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n        outputs = model(inputs)\n\n        sdr = calculate_sdr(targets, outputs)\n        sir = calculate_sir(targets, outputs, interference_signal=inputs)\n        sar = calculate_sar(targets, outputs, artifacts=outputs - targets)\n\n        final_sdr_list.append(sdr)\n        final_sir_list.append(sir)\n        final_sar_list.append(sar)\n\n# Compute mean metrics\nfinal_sdr = np.mean(final_sdr_list)\nfinal_sir = np.mean(final_sir_list)\nfinal_sar = np.mean(final_sar_list)\n\n# Save the model\nmodel_save_path = 'final_model.pth'\ntorch.save(model.state_dict(), model_save_path)\nConfig.LOGGER.info(f\"Model saved to {model_save_path}\")\n\n# Save the best hyperparameters\nhyperparams_save_path = 'best_hyperparameters.yaml'\nwith open(hyperparams_save_path, 'w') as f:\n    yaml.dump(best_params, f)\nConfig.LOGGER.info(f\"Hyperparameters saved to {hyperparams_save_path}\")\n\n# Save final metrics and loss\nmetrics_save_path = 'final_metrics.yaml'\nfinal_metrics = {\n    'final_loss': float(final_loss),  # Already a Python float\n    'final_sdr': float(final_sdr),    # Convert numpy.float64 to Python float\n    'final_sir': float(final_sir),    # Convert numpy.float64 to Python float\n    'final_sar': float(final_sar)     # Convert numpy.float64 to Python float\n}\nwith open(metrics_save_path, 'w') as f:\n    yaml.dump(final_metrics, f)\nConfig.LOGGER.info(f\"Final metrics saved to {metrics_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T12:58:39.724594Z","iopub.execute_input":"2024-08-25T12:58:39.725016Z","iopub.status.idle":"2024-08-25T13:01:17.869252Z","shell.execute_reply.started":"2024-08-25T12:58:39.724972Z","shell.execute_reply":"2024-08-25T13:01:17.868208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport yaml\nfrom torch.utils.data import DataLoader\n\n# Load the test dataset\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=Config.NUM_WORKERS)\n\n# Load the trained model\nmodel = MaskInference().to(Config.DEVICE)\nmodel.load_state_dict(torch.load('final_model.pth'))\nmodel.eval()\n\n# Initialize lists to store the metrics\ntest_loss = 0.0\nsdr_list, sir_list, sar_list = [], [], []\n\ncriterion = nn.MSELoss()\n\n# Evaluate the model on the test set\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n\n        # Generate predictions\n        outputs = model(inputs)\n\n        # Calculate loss\n        loss = criterion(outputs, targets)\n        test_loss += loss.item()\n\n        # Calculate metrics\n        sdr = calculate_sdr(targets, outputs)\n        sir = calculate_sir(targets, outputs, interference_signal=inputs)\n        sar = calculate_sar(targets, outputs, artifacts=outputs - targets)\n\n        # Append metrics to lists\n        sdr_list.append(sdr)\n        sir_list.append(sir)\n        sar_list.append(sar)\n\n# Calculate average metrics\ntest_loss /= len(test_loader)\nmean_sdr = np.mean(sdr_list)\nmean_sir = np.mean(sir_list)\nmean_sar = np.mean(sar_list)\n\n# Log the results\nConfig.LOGGER.info(f\"Test Loss: {test_loss:.4f}\")\nConfig.LOGGER.info(f\"Test SDR: {mean_sdr:.4f}\")\nConfig.LOGGER.info(f\"Test SIR: {mean_sir:.4f}\")\nConfig.LOGGER.info(f\"Test SAR: {mean_sar:.4f}\")\n\n# Save test metrics\ntest_metrics_save_path = 'test_metrics.yaml'\ntest_metrics = {\n    'test_loss': float(test_loss),\n    'test_sdr': float(mean_sdr),\n    'test_sir': float(mean_sir),\n    'test_sar':float( mean_sar)\n}\nwith open(test_metrics_save_path, 'w') as f:\n    yaml.dump(test_metrics, f)\nConfig.LOGGER.info(f\"Test metrics saved to {test_metrics_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:44:24.797290Z","iopub.execute_input":"2024-08-25T13:44:24.798025Z","iopub.status.idle":"2024-08-25T13:45:05.282994Z","shell.execute_reply.started":"2024-08-25T13:44:24.797993Z","shell.execute_reply":"2024-08-25T13:45:05.281844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport scipy.signal\nimport librosa\n\n# Define post-processing functions\n\ndef refine_mask(mask, method='wiener', size=3):\n    \"\"\"\n    Apply a post-processing refinement to the mask.\n\n    Args:\n    - mask (torch.Tensor): The predicted mask.\n    - method (str): The post-processing method. Options: 'median', 'wiener'.\n    - size (int): Size of the filter.\n\n    Returns:\n    - refined_mask (torch.Tensor): The refined mask.\n    \"\"\"\n    mask_np = mask.cpu().numpy()  # Convert to numpy for processing\n\n    if method == 'median':\n        refined_mask = scipy.signal.medfilt(mask_np, kernel_size=size)\n    elif method == 'wiener':\n        refined_mask = scipy.signal.wiener(mask_np, mysize=size)\n    else:\n        raise ValueError(\"Unsupported method. Choose 'median' or 'wiener'.\")\n\n    return torch.tensor(refined_mask, device=mask.device)\n\ndef apply_dynamic_range_compression(signal, threshold=0.5, ratio=4.0):\n    \"\"\"\n    Apply dynamic range compression to the separated source.\n\n    Args:\n    - signal (torch.Tensor): The audio signal.\n    - threshold (float): Threshold level above which compression is applied.\n    - ratio (float): Compression ratio.\n\n    Returns:\n    - compressed_signal (torch.Tensor): The compressed audio signal.\n    \"\"\"\n    signal_np = signal.cpu().numpy()\n\n    # Apply dynamic range compression\n    signal_compressed = np.where(signal_np > threshold,\n                                 threshold + (signal_np - threshold) / ratio,\n                                 signal_np)\n\n    return torch.tensor(signal_compressed, device=signal.device)\n\n# Load the trained model\nmodel = MaskInference().to(Config.DEVICE)\nmodel.load_state_dict(torch.load('final_model.pth'))\nmodel.eval()\n\n# Define a DataLoader for the test set\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=Config.NUM_WORKERS)\n\n# Lists to store post-processing results\ntest_sdrs = []\ntest_sirs = []\ntest_sars = []\n\n# Iterate over the test set and apply post-processing\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(Config.DEVICE), targets.to(Config.DEVICE)\n\n        # Model inference\n        output = model(inputs)\n\n        # Apply post-processing techniques\n        refined_mask = refine_mask(output, method='wiener', size=3)\n        output_refined = apply_dynamic_range_compression(refined_mask)\n\n        # Calculate SDR, SIR, SAR\n        sdr = calculate_sdr(targets, output_refined)\n        sir = calculate_sir(targets, output_refined, interference_signal=inputs)\n        sar = calculate_sar(targets, output_refined, artifacts=output_refined - targets)\n        \n        # Store results\n        test_sdrs.append(sdr)\n        test_sirs.append(sir)\n        test_sars.append(sar)\n\n        Config.LOGGER.info(f\"Processed batch - SDR: {sdr:.4f}, SIR: {sir:.4f}, SAR: {sar:.4f}\")\n\n# Compute mean values for test metrics\nmean_test_sdr = np.mean(test_sdrs)\nmean_test_sir = np.mean(test_sirs)\nmean_test_sar = np.mean(test_sars)\n\n# Log final test metrics\nConfig.LOGGER.info(f\"Final Test Metrics - SDR: {mean_test_sdr:.4f}, SIR: {mean_test_sir:.4f}, SAR: {mean_test_sar:.4f}\")\n\n# Optionally, log these results to Weights & Biases\nwandb.log({\n    \"Test SDR\": mean_test_sdr,\n    \"Test SIR\": mean_test_sir,\n    \"Test SAR\": mean_test_sar\n})\n\nprint(\"Post-processing and evaluation complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:02:01.308712Z","iopub.execute_input":"2024-08-25T13:02:01.309045Z","iopub.status.idle":"2024-08-25T13:03:06.986277Z","shell.execute_reply.started":"2024-08-25T13:02:01.309015Z","shell.execute_reply":"2024-08-25T13:03:06.985031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport torch\nimport numpy as np\nimport scipy.ndimage\nimport pywt\nfrom scipy.io.wavfile import write\nfrom scipy import signal\n\n# High-Pass Filter using scipy\ndef apply_high_pass_filter(audio, sr, cutoff=100):\n    sos = signal.iirfilter(2, cutoff, btype='highpass', ftype='butter', fs=sr, output='sos')\n    filtered_audio = signal.sosfilt(sos, audio)\n    return filtered_audio\n\n# Dynamic Range Compression\ndef apply_compression(audio, threshold=-20, ratio=4):\n    compressor = librosa.effects.preemphasis(audio, coef=threshold)\n    return compressor\n\n# Noise Reduction using Spectral Gating\ndef reduce_noise(audio, sr, noise_reduction_factor=1.5, n_fft=2048, hop_length=512, n_std_thresh=1.5):\n    \"\"\"\n    Reduces noise from audio using spectral gating.\n    \n    Parameters:\n    - audio: np.array, the input audio signal.\n    - sr: int, the sample rate of the audio.\n    - noise_reduction_factor: float, factor to determine noise threshold.\n    - n_fft: int, FFT window size.\n    - hop_length: int, hop length for STFT.\n    \n    Returns:\n    - reduced_audio: np.array, the denoised audio signal.\n    \"\"\"\n    # Step 1: Compute the STFT of the audio\n    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n    magnitude, phase = np.abs(stft), np.angle(stft)\n    \n    # Step 2: Estimate noise from the quietest frames\n    noise_estimate = np.mean(magnitude, axis=1, keepdims=True)\n\n    # Step 3: Create a noise threshold\n    noise_thresh = noise_estimate * noise_reduction_factor\n    \n    # Step 4: Apply spectral gating\n    mask = magnitude >= noise_thresh\n    gated_magnitude = magnitude * mask\n    \n    # Step 5: Reconstruct the denoised audio using inverse STFT\n    stft_denoised = gated_magnitude * np.exp(1j * phase)\n    reduced_audio = librosa.istft(stft_denoised, hop_length=hop_length)\n    \n    return reduced_audio\n\n# Spectrogram Augmentation (time masking)\ndef time_masking(spec, T=20):\n    time_mask = np.random.randint(0, spec.shape[1] - T)\n    spec[:, time_mask:time_mask + T] = 0\n    return spec\n\n# Smoothing Mask\ndef smooth_mask(mask, kernel_size=3):\n    smoothed_mask = scipy.ndimage.median_filter(mask.cpu().numpy(), size=kernel_size)\n    return torch.tensor(smoothed_mask, device=mask.device)\n\n# Wavelet Denoising\ndef wavelet_denoise(audio):\n    \"\"\"\n    Apply wavelet denoising to the audio signal.\n\n    Parameters:\n    - audio (np.ndarray): The input audio signal.\n\n    Returns:\n    - denoised_audio (np.ndarray): The denoised audio signal.\n    \"\"\"\n    coeffs = pywt.wavedec(audio, 'db1', level=2)\n    threshold = np.median(np.abs(coeffs[-1])) / 0.6745\n    denoised_coeffs = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs]\n    denoised_audio = pywt.waverec(denoised_coeffs, 'db1')\n    return denoised_audio\n\n# Dynamic Range Expansion\ndef apply_dynamic_range_expansion(audio, expansion_ratio=2):\n    expanded_audio = audio ** expansion_ratio\n    return expanded_audio\n\n# Load MP3 function\ndef load_mp3(file_path, target_sample_rate=44100):\n    \"\"\"Load an MP3 file and convert it to the target sample rate.\"\"\"\n    y, sr = librosa.load(file_path, sr=target_sample_rate, mono=False)\n    return y, sr\n\n# Preprocessing Function\ndef preprocess_audio(y, sr):\n    \"\"\"Preprocess the audio data to prepare it for the model.\"\"\"\n    y = apply_high_pass_filter(y, sr)\n    y = apply_compression(y)\n    y = reduce_noise(y, sr)\n\n    if y.ndim == 2:\n        y = librosa.to_mono(y)\n\n    stft = librosa.stft(y, n_fft=2048, hop_length=512)\n    magnitude = np.abs(stft)\n    phase = np.angle(stft)\n\n    magnitude_normalized = (magnitude - magnitude.mean()) / (magnitude.std() + 1e-6)\n    magnitude_normalized = time_masking(magnitude_normalized)  # Optional augmentation\n\n    # Double the frequency bins to match the model's expected input size\n    magnitude_doubled = np.concatenate([magnitude_normalized, magnitude_normalized], axis=0)\n\n    magnitude_tensor = torch.tensor(magnitude_doubled, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n    \n    return magnitude_tensor, phase\n\n# Post-Processing and Reconstruction\ndef reconstruct_audio(separated_magnitude, phase):\n    \"\"\"Reconstruct the audio waveform from the modified spectrogram.\"\"\"\n    phase_doubled = np.concatenate([phase, phase], axis=0)\n    complex_stft = separated_magnitude * np.exp(1j * phase_doubled)\n    y_reconstructed = librosa.istft(complex_stft, hop_length=512)\n    \n    # Apply wavelet denoising directly to the numpy array\n    y_reconstructed = wavelet_denoise(y_reconstructed)\n    y_reconstructed = apply_dynamic_range_expansion(y_reconstructed)\n    \n    return y_reconstructed\n\n# Save the audio file\ndef save_audio(file_path, audio_data, sample_rate):\n    \"\"\"Save the processed audio to a file.\"\"\"\n    write(file_path, sample_rate, (audio_data * 32767).astype(np.int16))\n\n# Remove vocals function (this is the core function you need to define)\ndef remove_vocals(model, magnitude_tensor):\n    \"\"\"\n    Use the trained model to remove vocals from the input spectrogram.\n    \n    Parameters:\n    - model: The trained PyTorch model.\n    - magnitude_tensor: The input magnitude spectrogram as a tensor.\n    \n    Returns:\n    - mask: The vocal mask predicted by the model.\n    \"\"\"\n    with torch.no_grad():\n        mask = model(magnitude_tensor)\n    return mask\n\n# Full Processing Pipeline\ndef process_audio(file_path, output_path, model):\n    y, sr = load_mp3(file_path)\n\n    # Preprocess the audio\n    magnitude_tensor, phase = preprocess_audio(y, sr)\n\n    # Ensure the magnitude tensor is on the same device as the model\n    magnitude_tensor = magnitude_tensor.to(Config.DEVICE)\n\n    # Remove vocals using the trained model\n    mask = remove_vocals(model, magnitude_tensor)\n\n    # Smooth the mask\n    smoothed_mask = smooth_mask(mask)\n\n    # Apply the mask to the magnitude spectrogram\n    separated_magnitude = magnitude_tensor.squeeze().cpu().numpy() * (1 - smoothed_mask.cpu().numpy())\n\n    # Reconstruct the waveform from the modified magnitude and original phase\n    reconstructed_waveform = reconstruct_audio(separated_magnitude, phase)\n\n    # Save the result as a WAV file\n    save_audio(output_path, reconstructed_waveform, sr)\n    print(f\"Vocal removal complete. The output audio is saved as '{output_path}'.\")\n\n# Example Usage\nfile_path = '/kaggle/input/example-song/Corey Taylor - Snuff (Acoustic).mp3'\noutput_path = '/kaggle/working/example_song_no_vocals.wav'\nprocess_audio(file_path, output_path, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:03:06.988964Z","iopub.execute_input":"2024-08-25T13:03:06.989401Z","iopub.status.idle":"2024-08-25T13:04:19.666405Z","shell.execute_reply.started":"2024-08-25T13:03:06.989357Z","shell.execute_reply":"2024-08-25T13:04:19.665327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install onnxruntime shap\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:04:19.667654Z","iopub.execute_input":"2024-08-25T13:04:19.668326Z","iopub.status.idle":"2024-08-25T13:04:34.118881Z","shell.execute_reply.started":"2024-08-25T13:04:19.668296Z","shell.execute_reply":"2024-08-25T13:04:34.117531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a test dataset loader ready\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# Get a single batch (mixture and target)\nfor batch in test_loader:\n    mixture, target = batch\n    mixture = mixture.to(Config.DEVICE)\n    target = target.to(Config.DEVICE)\n    break  # We only need one example for explanation\n\n# Now you have `mixture` and `target` defined\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:23:14.672967Z","iopub.execute_input":"2024-08-25T13:23:14.673710Z","iopub.status.idle":"2024-08-25T13:23:17.666201Z","shell.execute_reply.started":"2024-08-25T13:23:14.673677Z","shell.execute_reply":"2024-08-25T13:23:17.665242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# Define the function to compute saliency maps\ndef compute_saliency_maps(input_data, target_class):\n    # Ensure the model is in training mode to allow backward pass\n    model.train()\n\n    # Ensure the input requires gradients\n    input_data.requires_grad_()\n\n    # Forward pass\n    output = model(input_data)\n    \n    # Calculate the loss\n    loss = criterion(output, target_class)\n    \n    # Backward pass to compute gradients\n    loss.backward()\n    \n    # Get the absolute value of the gradients\n    saliency = input_data.grad.abs().detach().cpu().numpy()\n    \n    # Switch the model back to evaluation mode\n    model.eval()\n    \n    return saliency\n\n\ndef plot_saliency_map(saliency_map, title=\"Saliency Map\", save_path=None):\n    # Assuming saliency_map is 4D: (batch_size, channels, freq_bins, time_steps)\n    # Reduce it to 2D: (freq_bins, time_steps)\n    \n    # If there's only one batch and one channel, you can squeeze it to 2D\n    saliency_map = saliency_map.squeeze()  # This will reduce the dimensions\n    \n    if saliency_map.ndim == 3:  # If there are still 3 dimensions (e.g., channels remain)\n        saliency_map = saliency_map[0]  # Select the first channel or average/sum across channels\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(saliency_map, cmap='inferno', aspect='auto')\n    plt.colorbar(label='Saliency Intensity')\n    plt.title(title, fontsize=18, fontweight='bold')\n    plt.xlabel('Time Frames', fontsize=14)\n    plt.ylabel('Frequency Bins', fontsize=14)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    \n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n    else:\n        plt.show()\n\n\n       \n    # Assuming mixture and target are defined tensors\nsaliency_map = compute_saliency_maps(mixture.to(Config.DEVICE), target.to(Config.DEVICE))\n\n# Plot the saliency map\nplot_saliency_map(saliency_map, title=\"Saliency Map\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:29:40.893630Z","iopub.execute_input":"2024-08-25T13:29:40.894342Z","iopub.status.idle":"2024-08-25T13:29:41.414252Z","shell.execute_reply.started":"2024-08-25T13:29:40.894307Z","shell.execute_reply":"2024-08-25T13:29:41.413156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport yaml\n\n# Load final metrics from YAML file\nwith open('final_metrics.yaml', 'r') as f:\n    final_metrics = yaml.safe_load(f)\n\n# Load test metrics from YAML file\nwith open('test_metrics.yaml', 'r') as f:\n    test_metrics = yaml.safe_load(f)\n\n# Extract final (training) metrics\nfinal_sdr = final_metrics.get('final_sdr', 0)\nfinal_sir = final_metrics.get('final_sir', 0)\nfinal_sar = final_metrics.get('final_sar', 0)\nfinal_loss = final_metrics.get('final_loss', 0)\n\n# Extract test metrics\ntest_sdr = test_metrics.get('test_sdr', 0)\ntest_sir = test_metrics.get('test_sir', 0)\ntest_sar = test_metrics.get('test_sar', 0)\ntest_loss = test_metrics.get('test_loss', 0)\n\n# Prepare lists for plotting\nfinal_val_metrics = [final_sdr, final_sir, final_sar]\nfinal_test_metrics = [test_sdr, test_sir, test_sar]\nmetrics_labels = [\"SDR\", \"SIR\", \"SAR\"]\n\n# Plot comparison of SDR, SIR, and SAR between final (training) and test\ndef plot_metric_comparison(val_metrics, test_metrics, labels):\n    x = np.arange(len(labels))  # Label locations\n    width = 0.35  # Width of the bars\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    rects1 = ax.bar(x - width/2, val_metrics, width, label='Final (Training)', color='dodgerblue')\n    rects2 = ax.bar(x + width/2, test_metrics, width, label='Test', color='orange')\n\n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_ylabel('Metric (dB)', fontsize=14)\n    ax.set_title('Comparison of Final (Training) vs Test Metrics', fontsize=16, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, fontsize=12)\n    ax.legend(fontsize=12)\n\n    # Add value labels on top of bars\n    def add_labels(rects):\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate(f'{height:.2f}',\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', fontsize=12)\n\n    add_labels(rects1)\n    add_labels(rects2)\n\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.show()\n\n# Call the plotting function\nplot_metric_comparison(final_val_metrics, final_test_metrics, metrics_labels)\n\n# Plot loss comparison\ndef plot_loss_comparison(final_loss, test_loss):\n    losses = [final_loss, test_loss]\n    labels = ['Final (Training) Loss', 'Test Loss']\n    \n    plt.figure(figsize=(8, 5))\n    bars = plt.bar(labels, losses, color=['dodgerblue', 'orange'], edgecolor='black')\n    \n    # Add value labels on top of bars\n    for bar in bars:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f'{yval:.2f}', ha='center', fontsize=12)\n\n    plt.ylabel('Loss', fontsize=14)\n    plt.title('Final (Training) vs Test Loss', fontsize=16, fontweight='bold')\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.show()\n\n# Call the loss comparison plotting function\nplot_loss_comparison(final_loss, test_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:48:40.753043Z","iopub.execute_input":"2024-08-25T13:48:40.753530Z","iopub.status.idle":"2024-08-25T13:48:41.320447Z","shell.execute_reply.started":"2024-08-25T13:48:40.753493Z","shell.execute_reply":"2024-08-25T13:48:41.318975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Visualization Functions\ndef plot_loss_curves(train_losses, val_losses):\n    epochs = np.arange(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n    plt.title('Training and Validation Loss Over Epochs', fontsize=16, fontweight='bold')\n    plt.xlabel('Epochs', fontsize=14)\n    plt.ylabel('Loss', fontsize=14)\n    plt.legend(fontsize=12)\n    \n    # Force the x-axis to display only whole numbers\n    plt.xticks(ticks=epochs)\n    \n    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.show()\n\n# Example usage\nplot_loss_curves(train_losses, val_losses)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-25T13:52:13.041727Z","iopub.execute_input":"2024-08-25T13:52:13.042402Z","iopub.status.idle":"2024-08-25T13:52:13.330987Z","shell.execute_reply.started":"2024-08-25T13:52:13.042371Z","shell.execute_reply":"2024-08-25T13:52:13.329936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport onnx\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\n# Function to convert the PyTorch model to ONNX format\ndef convert_to_onnx_simple(model, input_tensor, output_path):\n    \"\"\"\n    Convert the PyTorch model to ONNX format.\n    \n    Parameters:\n    - model: The trained PyTorch model.\n    - input_tensor: A sample input tensor for tracing the model.\n    - output_path: The file path to save the ONNX model.\n    \n    Returns:\n    - None\n    \"\"\"\n    torch.onnx.export(\n        model,\n        input_tensor,\n        output_path,\n        export_params=True,\n        opset_version=12,\n        do_constant_folding=True,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n    )\n    print(f\"Model has been converted to ONNX and saved to {output_path}\")\n\n# Function to quantize the ONNX model\ndef quantize_onnx_model_simple(onnx_model_path, quantized_model_path):\n    \"\"\"\n    Quantize the ONNX model to reduce size and improve inference speed.\n    \n    Parameters:\n    - onnx_model_path: Path to the original ONNX model.\n    - quantized_model_path: Path to save the quantized ONNX model.\n    \n    Returns:\n    - None\n    \"\"\"\n    quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)\n    print(f\"Quantized model saved to {quantized_model_path}\")\n\n# Example usage of the full process\ndef full_process_simple(model, input_size, device):\n    \"\"\"\n    Run the full process including ONNX conversion and quantization.\n    \n    Parameters:\n    - model: The trained PyTorch model.\n    - input_size: A tuple representing the size of the input tensor.\n    - device: The device to which the tensors should be moved (e.g., 'cpu' or 'cuda').\n    \n    Returns:\n    - None\n    \"\"\"\n    # Move the model to the correct device\n    model.to(device)\n    \n    # Create a dummy input tensor with the correct dimensions for tracing\n    batch_size = 1\n    channels = 1\n    dummy_input = torch.randn(batch_size, channels, *input_size).to(device)\n\n    # Define paths\n    onnx_model_path = 'model.onnx'\n    quantized_model_path = 'model_quantized.onnx'\n\n    # Convert the model to ONNX format\n    convert_to_onnx_simple(model, dummy_input, onnx_model_path)\n\n    # Quantize the ONNX model\n    quantize_onnx_model_simple(onnx_model_path, quantized_model_path)\n\n# Assuming you have a model and input size ready\ninput_size = (2050, 23482)  # Frequency bins x Time steps\nmodel.eval()  # Set model to evaluation mode\n\n# Specify the device (e.g., 'cuda' for GPU, 'cpu' for CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Run the full process\nfull_process_simple(model, input_size, device)\n","metadata":{},"execution_count":null,"outputs":[]}]}